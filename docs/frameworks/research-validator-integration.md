# Research Validator - Integration in Product Management Kit

**Version:** 1.0
**Last Updated:** 2025-12-09
**Purpose:** Lokales AI Response Validation Tool fÃ¼r kritische Research-Entscheidungen

---

## Executive Summary

**Was:** Lokales Validierungs-Tool das mehrere AI-Responses vergleicht und kritisch analysiert.

**Warum:** Verhindert Bias, WidersprÃ¼che und blinde Flecken in Business Research.

**Wie:** Abliterated Local Model (Ollama + Dolphin-Mistral) analysiert gesammelte AI-Responses offline.

**Wann:** PrimÃ¤r in **Base Research Review Phase** + ad-hoc bei kritischen Questions in Iterations.

---

## Core Concept

### Problem

Wenn du AI (Claude, GPT-4, Gemini) fÃ¼r Research nutzt:
- âœ… Schnell, gÃ¼nstig, hilft bei Ideenfindung
- âš ï¸ **ABER:** Alle haben Ã¤hnliche Biases (gemeinsame Training-Daten, RLHF)
- âš ï¸ **ABER:** Alle sind auf "helpful" konditioniert (niceness bias)
- âš ï¸ **ABER:** WidersprÃ¼che zwischen Modellen bleiben unentdeckt
- âš ï¸ **ABER:** Blinde Flecken (was ALLE ignorieren) sind unsichtbar

### LÃ¶sung

**Research Validator:**
1. Du fragst **mehrere AIs** die gleiche Research-Frage (manuell)
2. Speicherst jede Response als `.md` File
3. **Lokales abliterated Model** analysiert alle Responses
4. Validator findet:
   - **Contradictions** (wo widersprechen sich Modelle?)
   - **Consensus Bias** (was sagen ALLE? = verdÃ¤chtig)
   - **Blind Spots** (was erwÃ¤hnt NIEMAND?)
   - **Premortem** (forciert Failure Scenarios)
5. Du nutzt Validation Report fÃ¼r bessere Decisions

---

## Integration in dein Kit

### Wo passt das hin?

**PrimÃ¤re Integration: Base Research Review Phase**

```
Phase 0: Base Research (6 Kategorien)
  â†“
Phase 2: Base Research Review â† VALIDATOR HIER! â­
  â†’ Cross-Category Consistency Check
  â†’ Big Picture Questions
  â†’ Validation kritischer Annahmen â† VALIDATOR
  â†“
Phase 3: Question Extraction
  â†“
Phase 4: GIST Iterations â† VALIDATOR auch hier (bei Bedarf)
  â†“
Phase 5: Output
```

**Warum Review Phase?**
- âœ… Du hast alle 6 Kategorien researched = vollstÃ¤ndiger Kontext
- âœ… Vor Planning = Verhindert falsche Decisions basierend auf Bias
- âœ… Big Picture Check = Validator entdeckt Inkonsistenzen
- âœ… Timeboxed = passt zu Review-Phase Zeitrahmen (1-2 Tage)

**SekundÃ¤r: Ad-hoc in Iterations**
- Wenn eine Question besonders kritisch ist (z.B. "Local vs Hybrid?")
- Nutze Validator fÃ¼r tiefere Analyse
- Dokumentiere in Iteration File unter "Validation"

---

## Workflow: Validator in Review Phase

### Vorbereitung (einmalig)

```bash
# 1. Ollama installieren (einmalig)
curl -fsSL https://ollama.com/install.sh | sh

# 2. Abliterated Model holen (einmalig)
ollama pull dolphin-mistral

# 3. Python Environment (einmalig)
python -m venv product-management/validator-env
source product-management/validator-env/bin/activate
pip install requests

# 4. Validator Code (Claude Code erstellt spÃ¤ter)
# â†’ product-management/tools/validator/
```

### Usage wÃ¤hrend Review Phase

**Schritt 1: Review Progress (wie gehabt)**
```markdown
2-working-state/planning_state.json:
{
  "current": {
    "phase": "0-base-research-review",
    "review_progress": {
      "1-technical-feasibility": "in_progress",
      ...
    }
  }
}
```

**Schritt 2: Critical Assumptions identifizieren**

WÃ¤hrend du jede Kategorie reviewst, markiere **kritische Annahmen**:

```markdown
# In 2-working-state/0-base-research/1-technical-feasibility/tech-stack.md

## CRITICAL ASSUMPTION (for Validation):
"React (32/40), Vue (28/40), Svelte (25/40)"
- Source: AI analysis (Claude + GPT-4)
- Confidence: MEDIUM
- Impact: HIGH (affects dev velocity, hiring, bundle size)
- **â†’ VALIDATE THIS**
```

**Schritt 3: AI Responses sammeln (manuell)**

FÃ¼r jede kritische Assumption:

1. Frage **mindestens 3 verschiedene AIs:**
   - Claude (claude.ai)
   - GPT-4 (ChatGPT)
   - Gemini (AI Studio)
   - Optional: Lokales Model (Ollama)

2. **Gleiche Frage, verschiedene AIs:**
   ```
   "Bewerte React, Vue, Svelte fÃ¼r ein Desktop-App Projekt:
   - Team: 1-2 Entwickler
   - Constraints: Bundle size wichtig
   - Requirements: Desktop (Tauri)

   Bewerte jedes Framework 1-10 in:
   - Dev Velocity, Community Support, Bundle Size, Tauri Integration"
   ```

3. **Speichere Responses:**
   ```
   2-working-state/research-data/validations/tech-stack-framework/
   â”œâ”€â”€ question.md          (Die Frage)
   â”œâ”€â”€ claude-response.md   (Claude's Antwort)
   â”œâ”€â”€ gpt4-response.md     (GPT-4's Antwort)
   â”œâ”€â”€ gemini-response.md   (Gemini's Antwort)
   â””â”€â”€ llama-response.md    (Optional: Llama)
   ```

**Schritt 4: Validator laufen lassen**

```bash
cd product-management/tools/validator

python validator.py \
  ../../2-working-state/research-data/validations/tech-stack-framework/question.md \
  ../../2-working-state/research-data/validations/tech-stack-framework/ \
  ../../2-working-state/research-data/validations/tech-stack-framework/

# Output: validation-report-TIMESTAMP.md
```

**Schritt 5: Validation Report analysieren**

```markdown
# Validation Report - Tech Stack Framework

**Date:** 2025-12-09
**Question:** Framework Choice (React vs Vue vs Svelte)
**Models Analyzed:** claude, gpt4, gemini, llama
**Validator:** dolphin-mistral (local)

---

## âš”ï¸ Contradictions

**CONTRADICTION 1: Bundle Size**
- **claude.md** sagt: "Svelte 15KB, React 45KB"
- **gpt4.md** sagt: "Svelte 7KB, React 140KB"
- **Implikation:** Faktor 2-3 Unterschied - welche Messung? Production oder Dev?

**CONTRADICTION 2: Tauri Integration**
- **claude.md** sagt: "All equal for Tauri"
- **gemini.md** sagt: "Vue better Tauri integration (official examples)"
- **Implikation:** Muss nachprÃ¼fen - gibt es offizielle Tauri+Vue Examples?

---

## ğŸ¯ Consensus Bias

**CONSENSUS 1:** Alle Modelle sagen "React has largest community"
- **Warum verdÃ¤chtig:** Standard-Narrative, aber ist es fÃ¼r Tauri relevant?
- **Alternative:** Tauri community kÃ¶nnte Vue/Svelte bevorzugen
- **Action:** Check Tauri showcase apps - welche Frameworks nutzen sie?

---

## ğŸ‘ï¸ Blind Spots

Folgende Aspekte erwÃ¤hnt KEIN Model:

1. **HMR Performance** - Hot Module Reload bei Desktop Apps
2. **Memory Footprint** - RAM usage on user devices (8GB constraint)
3. **Update Size** - Incremental update size (fÃ¼r Auto-updater)
4. **Tauri Plugin Ecosystem** - Framework-specific plugins

---

## âš°ï¸ Premortem

Es ist 2026. Das Projekt ist gescheitert. Post-Mortem:

**Haupt-Todesursache:** Bundle size zu groÃŸ â†’ App zu langsam â†’ User churn.

**Ignorierte Warnsignale:**
- React bundle trotz Optimierung 80KB (vs. 15KB bei Svelte)
- Langsame Startup-Zeit auf Ã¤lteren Devices
- Update-Downloads 50MB statt 5MB

**Fundamentale Fehlannahme:** "React community = besserer Support" - aber Tauri community nutzt primÃ¤r Vue/Svelte.

---

## ğŸ“Š Summary

**Verdict:** INVESTIGATE FURTHER

**Critical Actions:**
- [ ] KlÃ¤re Bundle Size Widerspruch (messen, nicht glauben)
- [ ] Check Tauri showcase apps (welche Frameworks?)
- [ ] Test HMR performance (alle 3 Frameworks)
- [ ] Memory profiling (8GB constraint)

**Confidence:** LOW â†’ Zu viele WidersprÃ¼che fÃ¼r Decision
```

**Schritt 6: Actions umsetzen**

Basierend auf Validation Report:
1. Fehlende Infos researchen (Blind Spots)
2. WidersprÃ¼che klÃ¤ren (selbst testen/messen)
3. Consensus-Annahmen hinterfragen

**Schritt 7: Base Research updaten**

```markdown
# In 2-working-state/0-base-research/1-technical-feasibility/tech-stack.md

## UPDATE nach Validation (2025-12-09)

**Validation fand:**
- âš ï¸ Widerspruch: Bundle Size Messungen variieren 2-3x
- âš ï¸ Blind Spot: HMR Performance, Memory Footprint
- âœ… Consensus: React community grÃ¶ÃŸer (validiert durch GitHub stars)

**Actions taken:**
- âœ… Eigene Bundle Size Messung: React 42KB, Vue 30KB, Svelte 12KB (production build)
- âœ… Tauri Showcase check: 60% Vue, 30% React, 10% Svelte
- âœ… Memory profiling: Svelte 50MB, Vue 60MB, React 80MB (initial load)

**Updated Scores:**
- React: 28/40 (down from 32, bundle size issue)
- Vue: 32/40 (up from 28, Tauri ecosystem + balance)
- Svelte: 30/40 (up from 25, bundle + memory wins)

**Confidence:** MEDIUM â†’ HIGH (nach Validation + eigenen Tests)
```

**Schritt 8: Dokumentiere Validation**

```markdown
# In 2-working-state/planning_state.json

{
  "current": {
    "phase": "0-base-research-review",
    "validations_run": [
      {
        "topic": "tech-stack-framework",
        "date": "2025-12-09",
        "status": "complete",
        "outcome": "Updated scores, increased confidence",
        "report": "research-data/validations/tech-stack-framework/validation-report-20251209.md"
      }
    ]
  }
}
```

---

## Validation Methods (im Detail)

### 1. Contradiction Detection

**Was:** Findet faktische WidersprÃ¼che zwischen AI-Responses.

**Validator Prompt:**
```markdown
Analysiere diese AI Responses auf WIDERSPRÃœCHE:

{alle_responses}

Liste konkrete WidersprÃ¼che:
1. Welche faktischen Claims widersprechen sich?
2. Wo sind EinschÃ¤tzungen diametral unterschiedlich?

Format:
WIDERSPRUCH 1:
- Model X sagt: [Zitat]
- Model Y sagt: [Zitat]
- Implikation: [Was bedeutet das fÃ¼r Entscheidung?]
```

**Output-QualitÃ¤t:** Hoch - abliterated Model ist nicht "helpful", findet echte Konflikte.

---

### 2. Consensus Bias Detection

**Was:** Findet was ALLE Modelle sagen = verdÃ¤chtig (gemeinsame Training-Bias).

**Validator Prompt:**
```markdown
Finde CONSENSUS BIAS:

{alle_responses}

Was sagen ALLE Modelle? Das ist verdÃ¤chtig weil:
- Gemeinsame Training-Daten
- Ã„hnliche RLHF
- Kultureller Bias

FÃ¼r jeden Consensus-Punkt:
- Was sagen alle?
- Warum kÃ¶nnte das falsch sein?
- Welche Alternative wird ignoriert?
```

**Beispiel:**
- Alle: "React hat die grÃ¶ÃŸte Community"
- Validator: "Warum ist das fÃ¼r Tauri relevant? Tauri-spezifische Community kÃ¶nnte anders aussehen."

---

### 3. Blind Spot Detection

**Was:** Findet was NIEMAND erwÃ¤hnt = fehlende Perspektiven.

**Validator Prompt:**
```markdown
Gegeben die Frage:
{question}

Analysiere diese Responses:
{alle_responses}

Was fehlt in ALLEN Antworten?
- Wichtige Aspekte nicht erwÃ¤hnt
- Perspektiven ignoriert
- Trade-offs nicht diskutiert

Liste fehlende Aspekte die fÃ¼r die Entscheidung relevant sein kÃ¶nnten.
```

**Output:** Liste von Aspekten zum weiteren Research.

---

### 4. Premortem Analysis

**Was:** Forciert konkrete Failure Scenarios (bricht "helpful" Bias).

**Validator Prompt:**
```markdown
Es ist 2026. Das Projekt ist gescheitert. Post-Mortem Analyse:

Gegeben die Frage:
{question}

Und diese Antworten:
{alle_responses}

Schreibe ein detailliertes Failure Scenario:
- Haupt-Todesursache
- Ignorierte Warnsignale
- Fundamentale Fehlannahme
- Was hÃ¤tte man anders machen mÃ¼ssen?

Sei brutal ehrlich. Keine PlatitÃ¼den.
```

**Power:** Abliterated Model nimmt "Es ist gescheitert" als Fakt, ist nicht optimistisch.

---

### 5. Adversarial Prompting (manuell von dir)

**Was:** Du fragst bewusst verschiedene Perspektiven.

**Beispiel (Framework Choice):**

**Prompt 1 (Optimist):**
```
"Als React-Fan: Warum ist React die beste Wahl fÃ¼r unsere Tauri-App?"
```

**Prompt 2 (Pessimist):**
```
"Als Bundle-Size-Purist: Warum sollten wir React NICHT nutzen?"
```

**Prompt 3 (Neutral):**
```
"Faktenbasierte Bewertung: React vs Vue vs Svelte fÃ¼r Tauri Desktop App."
```

**Validator vergleicht:** Wie stark sind die Biases? Was sagen alle 3 Perspectives?

---

### 6. Anonymization (optional)

**Was:** Entfernt "ich/mein/wir" aus deiner Frage â†’ macht AI distanzierter/kritischer.

**Beispiel:**

âŒ **Original:**
```
"Ich plane eine AI Tutoring App. Sollte ich React oder Vue nutzen?"
```

âœ… **Anonymized:**
```
"Ein Entwickler plant eine AI Tutoring App. Sollte React oder Vue genutzt werden?"
```

**Warum:** AI ist weniger "helpful" wenn es 3rd-person ist (behandelt es objektiver).

**Implementation:** Simple String-Replacement im Validator-Script.

---

## File Structure

```
product-management/
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ validator/
â”‚       â”œâ”€â”€ validator.py         (Haupt-Logik)
â”‚       â”œâ”€â”€ cli.py               (Command Line Interface)
â”‚       â”œâ”€â”€ config.py            (Ollama Settings)
â”‚       â”œâ”€â”€ prompts.py           (Validation Prompts)
â”‚       â”œâ”€â”€ README.md            (Setup & Usage)
â”‚       â””â”€â”€ requirements.txt     (nur: requests)
â”‚
â”œâ”€â”€ 2-working-state/
â”‚   â”œâ”€â”€ research-data/
â”‚   â”‚   â””â”€â”€ validations/         â† Validation Data
â”‚   â”‚       â”œâ”€â”€ tech-stack-framework/
â”‚   â”‚       â”‚   â”œâ”€â”€ question.md
â”‚   â”‚       â”‚   â”œâ”€â”€ claude-response.md
â”‚   â”‚       â”‚   â”œâ”€â”€ gpt4-response.md
â”‚   â”‚       â”‚   â”œâ”€â”€ gemini-response.md
â”‚   â”‚       â”‚   â”œâ”€â”€ llama-response.md
â”‚   â”‚       â”‚   â””â”€â”€ validation-report-20251209.md
â”‚   â”‚       â”‚
â”‚   â”‚       â”œâ”€â”€ local-vs-hybrid/
â”‚   â”‚       â”‚   â”œâ”€â”€ question.md
â”‚   â”‚       â”‚   â”œâ”€â”€ ...
â”‚   â”‚       â”‚   â””â”€â”€ validation-report-20251209.md
â”‚   â”‚       â”‚
â”‚   â”‚       â””â”€â”€ pricing-model/
â”‚   â”‚           â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ planning_state.json     (tracks validations_run)
â”‚
â””â”€â”€ 1-description/
    â””â”€â”€ research-validator-integration.md  (dieses File)
```

---

## When to Use Validator?

### HIGH Priority (must validate)

âœ… **Architecture Decisions** (z.B. Local vs Hybrid)
- Reach: 10, Impact: 10
- Beeinflusst alles (Privacy, Performance, Cost)
- Multiple AIs widersprechen sich stark

âœ… **Market Sizing** (z.B. TAM/SAM/SOM)
- Impact: 10
- Zahlen variieren massiv zwischen Sources
- Consensus Bias (alle zitieren gleichen Report)

âœ… **Pricing Strategy** (z.B. Freemium vs Paid)
- Impact: 9-10
- Business-kritisch
- Viele Meinungen, wenig Fakten

### MEDIUM Priority (consider validation)

ğŸŸ¡ **Feature Prioritization** (z.B. MVP Scope)
- Impact: 8-9
- Nutze Validator wenn unklar

ğŸŸ¡ **GTM Channel Priority** (z.B. Reddit vs HackerNews vs Forums)
- Impact: 7-8
- Validation hilft bei blinden Flecken

### LOW Priority (skip validation)

âšª **Nice-to-Have Features** (z.B. Dark Mode)
- Impact: 3-5
- Nicht kritisch, spare Zeit

âšª **Naming/Branding** (z.B. Logo Colors)
- Impact: 4-6
- Subjektiv, Validation wenig Wert

---

## Cost & Time Budget

### Setup (einmalig)
- Ollama Installation: 10 min
- Model Download: 5 min (Dolphin-Mistral ~4GB)
- Python Environment: 5 min
- **Total:** 20 min

### Per Validation
- AI Responses sammeln (manuell): 20-30 min (3-4 AIs fragen)
- Responses als Files speichern: 5 min
- Validator laufen lassen: 2-5 min (lokal)
- Report analysieren: 10-15 min
- Actions umsetzen: 30-60 min (abhÃ¤ngig von Findings)
- **Total:** 1-2 Stunden pro kritischer Assumption

### Review Phase Budget
- Identifiziere 3-5 kritische Assumptions (aus 6 Kategorien)
- 1-2 Stunden pro Assumption
- **Total:** 3-10 Stunden fÃ¼r komplette Review Validation

**â†’ Fits in 1-2 Tage Review Phase Budget**

---

## Tech Stack (Validator)

### Required
- **Ollama:** Lokaler LLM Server (free, open-source)
- **Dolphin-Mistral:** Abliterated Mistral-7B (free, no censorship)
- **Python 3.8+:** Scripting
- **requests:** HTTP library (fÃ¼r Ollama API)

### Optional (Future)
- **Chroma/FAISS:** Vector DB fÃ¼r RAG (wenn du externe Daten einbinden willst)
- **LangChain:** Orchestration (wenn komplexere Workflows)
- **Gradio:** Web UI (wenn du GUI statt CLI willst)

**Start Simple:** CLI + Ollama + Python requests reicht!

---

## Alternative Models (falls Dolphin-Mistral nicht passt)

### Abliterated Models (keine RLHF "niceness")
1. **Dolphin-Mistral** (empfohlen)
   - Size: 7B params (~4GB)
   - Speed: ~30-50 tok/s (GPU), ~10-15 tok/s (CPU)
   - Lizenz: Apache 2.0

2. **Dolphin-Llama3:8b**
   - Size: 8B params (~5GB)
   - Speed: ~35-50 tok/s (GPU)
   - Lizenz: Llama 3 (commercial OK)

3. **Nous-Hermes-Uncensored**
   - Size: 7B/13B params
   - Speed: ~20-40 tok/s
   - Lizenz: Apache 2.0

### Standard Models (falls abliterated zu kritisch)
- **Llama 3.1:8b** (balanced)
- **Mistral:7b** (balanced)
- **Qwen2.5:7b** (good reasoning)

**Recommendation:** Start mit Dolphin-Mistral, wechseln wenn zu "brutal".

---

## Integration in Iterations (Optional)

Wenn eine **Question besonders kritisch** ist (z.B. I001 "Local vs Hybrid?"):

### In Iteration File:

```markdown
# I001: Local vs Hybrid Architecture?

**Question:** Should we build local-first or hybrid architecture?
**Priority:** CRITICAL (RICE 16.0)
**Status:** In Progress

---

## 1. Research

**Relevant Base Research:**
- 1-technical-feasibility/architecture.md
- 2-market-opportunity/segments.md (privacy segment)
- 3-monetization/cost-structure.md (cloud costs)

---

## 2. Validation â­ NEW SECTION

**Why Validate:**
- Decision impacts EVERYTHING (privacy, performance, cost, GTM)
- Multiple AI responses contradict each other
- High uncertainty (Confidence: 6/10)

**Validation Process:**
1. âœ… Collected 4 AI responses (Claude, GPT-4, Gemini, Llama)
2. âœ… Ran Validator (dolphin-mistral)
3. âœ… Analyzed report

**Validation Report:** `research-data/validations/local-vs-hybrid/validation-report-20251209.md`

**Key Findings:**
- âš ï¸ Contradiction: Cloud cost estimates vary 10x ($0.01 vs $0.10 per query)
- âš ï¸ Blind Spot: Installation friction for local-first (nobody mentioned)
- âœ… Consensus: Privacy = competitive moat (validated)

**Actions Taken:**
- âœ… Researched actual cloud costs (OpenAI, Anthropic pricing)
- âœ… Surveyed local AI tools (LM Studio, GPT4All) for installation UX
- âœ… Updated confidence: 6/10 â†’ 8/10

---

## 3. Goals

[... rest of GIST iteration]
```

---

## Best Practices

### Do's âœ…

âœ… **Validate Critical Decisions** - nicht alles, nur Impact 9-10
âœ… **Sammle 3-4 AI Responses** - mehr = bessere Widerspruchserkennung
âœ… **Gleiche Frage, gleicher Kontext** - faire Vergleichbarkeit
âœ… **Dokumentiere Actions** - was hast du nach Validation getan?
âœ… **Update Base Research** - Validation-Findings zurÃ¼ck ins Research
âœ… **Timeboxing** - 1-2 Stunden pro Validation, nicht lÃ¤nger

### Don'ts âŒ

âŒ **Nicht alles validieren** - nur kritische Assumptions (spare Zeit)
âŒ **Nicht blind vertrauen** - Validator kann auch falsch liegen
âŒ **Nicht ohne Follow-up** - Validation ohne Action = wertlos
âŒ **Nicht nur ein Model** - minimum 3 AI Responses fÃ¼r gute Validation
âŒ **Nicht in Analyse-Paralyse** - nach 2h Actions starten, nicht 2 Tage researchen

---

## Exit Criteria: Validation Complete

FÃ¼r eine Validation:
- [ ] Kritische Assumption identifiziert
- [ ] 3-4 AI Responses gesammelt
- [ ] Validator gelaufen
- [ ] Report analysiert
- [ ] Contradictions geklÃ¤rt (durch eigenes Research/Testing)
- [ ] Blind Spots adressiert (recherchiert oder dokumentiert fÃ¼r spÃ¤ter)
- [ ] Base Research updated mit Findings
- [ ] Confidence Level aktualisiert

FÃ¼r Review Phase (gesamt):
- [ ] 3-5 kritische Assumptions validiert
- [ ] Alle 6 Kategorien reviewed (mit oder ohne Validation)
- [ ] Cross-dependencies geprÃ¼ft
- [ ] planning_state.json updated
- [ ] Bereit fÃ¼r Question Extraction

---

## Future Enhancements

### Phase 1 (nice to have)
- **Web UI:** Gradio interface statt CLI
- **Auto-Prompting:** Script fragt AIs automatisch (via APIs)
- **RAG Integration:** Validator nutzt deine `research-data/` Files

### Phase 2 (advanced)
- **Fine-Tuned Validator:** Trainiert auf deinen Validation History
- **Multi-Validator:** Mehrere lokale Models fÃ¼r Cross-Validation
- **Validation Templates:** Vorgefertigte Prompts fÃ¼r hÃ¤ufige Validation-Types

**Start Simple:** CLI + manuelles Response-Sammeln reicht erstmal!

---

## Related Files

- **planning-framework.md** - GIST Iterations
- **research-framework.md** - Base Research Process
- **templates.md** - Template fÃ¼r Validation Report
- **planning_state.json** - Tracking validations_run

---

## Summary

**Validator = Research Quality Assurance Tool**

**Use Cases:**
1. âœ… Base Research Review (primÃ¤r)
2. âœ… Kritische Questions in Iterations (sekundÃ¤r)
3. âœ… WidersprÃ¼che klÃ¤ren
4. âœ… Blinde Flecken finden
5. âœ… Consensus Bias aufdecken

**Tech:** Lokal, kostenlos, keine APIs, privacy-safe

**ROI:** 1-2 Stunden Aufwand â†’ Verhindert falsche Decisions (â‚¬1000+ Impact)

**Integration:** Passt in existierende Review Phase (kein neuer Workflow nÃ¶tig)

---

**Next Steps:**

1. **Entscheide:** Willst du Validator nutzen? (Ja/Nein/SpÃ¤ter)
2. **Setup:** Ollama + Dolphin-Mistral installieren (20 min)
3. **Test:** Eine Validation durchfÃ¼hren (z.B. "Local vs Hybrid?")
4. **Evaluate:** Hilft es? Dann in Review Phase integrieren
5. **Iterate:** Bei Bedarf Prompts/Workflow anpassen

**Claude Code kann helfen mit:** Validator Code schreiben, Setup automatisieren, Prompts optimieren

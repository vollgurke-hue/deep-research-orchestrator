# Context Window Monitoring System

**Datum**: 2026-01-02
**Status**: Konzept ‚Üí Implementierung

---

## üéØ Ziel

Vollst√§ndige Transparenz √ºber Context-Nutzung w√§hrend der Research-Execution:
- Welche Daten werden in den Prompt geladen?
- Wie viele Tokens pro Context-Teil?
- Wie viel Platz ist noch im Context-Fenster?
- Wann besteht Halluzinations-Gefahr?

---

## üìä Context Window Struktur

### Komponenten eines Prompts

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. System Prompt                        ‚îÇ (statisch, ~500 tokens)
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 2. Technique Prompt Template            ‚îÇ (statisch, ~200-800 tokens)
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 3. Previous Phase Outputs               ‚îÇ (dynamisch, 0-10k tokens)
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 4. Current Phase Context                ‚îÇ (dynamisch, 0-5k tokens)
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 5. User Input / Query                   ‚îÇ (dynamisch, ~100-2k tokens)
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 6. Examples (optional)                  ‚îÇ (statisch, ~500 tokens)
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 7. Output Buffer (reserved for answer)  ‚îÇ (reserved, ~2k-8k tokens)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üö¶ Ampel-System f√ºr Token Usage

### Gr√ºn (Safe Zone): 0-60% des Context Window
```
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  40% (4096 / 10240 tokens)
‚úÖ Safe - Genug Platz f√ºr Output
```

### Gelb (Warning Zone): 60-85% des Context Window
```
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  75% (7680 / 10240 tokens)
‚ö†Ô∏è  Warning - Context wird knapp, Output k√∂nnte gek√ºrzt werden
```

### Rot (Danger Zone): 85-100% des Context Window
```
‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë  92% (9420 / 10240 tokens)
üö® Danger - Halluzinations-Gefahr! Context reduzieren!
```

---

## üìê Modell-Spezifische Limits

```json
{
  "models": {
    "tier1_fast": {
      "model_id": "mistral-7b-instruct",
      "context_window": 8192,
      "safe_limit": 4915,     // 60%
      "warning_limit": 6963,   // 85%
      "output_buffer": 2048    // Reserved for answer
    },
    "tier2_quality": {
      "model_id": "mixtral-8x7b-instruct",
      "context_window": 32768,
      "safe_limit": 19661,
      "warning_limit": 27853,
      "output_buffer": 4096
    },
    "tier3_deep": {
      "model_id": "llama-70b-instruct",
      "context_window": 4096,
      "safe_limit": 2458,
      "warning_limit": 3482,
      "output_buffer": 1024
    }
  }
}
```

---

## üîç Context Loading Anzeige

### W√§hrend der Execution

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üîÑ Loading Context for: Phase 0 ‚Üí Workflow 2 ‚Üí Step 3    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                           ‚îÇ
‚îÇ ‚úÖ System Prompt                           482 tokens    ‚îÇ
‚îÇ ‚úÖ Technique: market_need_detector         356 tokens    ‚îÇ
‚îÇ ‚úÖ Previous: competitor_analysis          2104 tokens    ‚îÇ
‚îÇ ‚úÖ Previous: customer_interviews          1823 tokens    ‚îÇ
‚îÇ ‚úÖ User Query: "AI tutoring app"           142 tokens    ‚îÇ
‚îÇ ‚è≥ Loading: Examples...                                   ‚îÇ
‚îÇ                                                           ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Total Context:                            4907 / 8192    ‚îÇ
‚îÇ ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë  60% ‚úÖ Safe           ‚îÇ
‚îÇ                                                           ‚îÇ
‚îÇ Output Buffer Reserved:                   2048 tokens    ‚îÇ
‚îÇ Estimated Max Output:                     ~1500 words    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üîÑ Iteration State Tracking

### Phase 1 (Excurse) - Iterative Loop

```json
{
  "iteration_state": {
    "current_phase": "phase_1_excurse",
    "current_iteration": 2,
    "max_iterations": 5,
    "iteration_history": [
      {
        "iteration": 1,
        "gaps_detected": [
          "Missing competitor pricing data",
          "No user retention metrics"
        ],
        "techniques_executed": ["gap_detector", "quick_web_research"],
        "context_loaded": {
          "previous_outputs": ["phase_0_output"],
          "tokens": 3420
        },
        "result": "gaps_found"
      },
      {
        "iteration": 2,
        "gaps_detected": [],
        "techniques_executed": ["validation_check"],
        "context_loaded": {
          "previous_outputs": ["phase_0_output", "iteration_1_output"],
          "tokens": 5840
        },
        "result": "complete"
      }
    ]
  }
}
```

### UI Visualization

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üîÑ Phase 1: Excurse (Iteration 2 / 5)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                         ‚îÇ
‚îÇ Iteration 1: ‚úÖ Complete                                ‚îÇ
‚îÇ   ‚îú‚îÄ Gaps Detected: 2                                   ‚îÇ
‚îÇ   ‚îú‚îÄ Context Size: 3420 tokens                          ‚îÇ
‚îÇ   ‚îî‚îÄ Duration: 12.4s                                    ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ Iteration 2: ‚è≥ In Progress...                          ‚îÇ
‚îÇ   ‚îú‚îÄ Current Step: validation_check                     ‚îÇ
‚îÇ   ‚îú‚îÄ Context Size: 5840 tokens (71% üü°)                 ‚îÇ
‚îÇ   ‚îî‚îÄ Elapsed: 8.2s                                      ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ Estimated Remaining: 0-3 iterations                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Implementierung

### Backend: Token Counting

```python
# src/utils/token_counter.py

import tiktoken

class TokenCounter:
    def __init__(self, model_name: str):
        # Use appropriate encoding for model
        self.encoding = tiktoken.encoding_for_model(model_name)

    def count_tokens(self, text: str) -> int:
        """Count tokens in text."""
        return len(self.encoding.encode(text))

    def estimate_context_size(self, prompt_parts: dict) -> dict:
        """
        Estimate total context size from parts.

        Args:
            prompt_parts: {
                "system_prompt": str,
                "technique_prompt": str,
                "previous_outputs": [str],
                "user_input": str,
                "examples": str
            }

        Returns:
            {
                "breakdown": {
                    "system_prompt": 482,
                    "technique_prompt": 356,
                    "previous_outputs": 3927,
                    "user_input": 142,
                    "examples": 0
                },
                "total": 4907,
                "percentage": 59.9
            }
        """
        breakdown = {}
        total = 0

        for key, value in prompt_parts.items():
            if isinstance(value, list):
                tokens = sum(self.count_tokens(v) for v in value)
            else:
                tokens = self.count_tokens(value) if value else 0

            breakdown[key] = tokens
            total += tokens

        return {
            "breakdown": breakdown,
            "total": total,
            "percentage": (total / self.get_context_limit()) * 100
        }

    def get_context_limit(self) -> int:
        """Get context window limit for model."""
        # Model-specific limits
        limits = {
            "mistral-7b-instruct": 8192,
            "mixtral-8x7b-instruct": 32768,
            "llama-70b-instruct": 4096
        }
        return limits.get(self.model_name, 4096)

    def get_safety_status(self, token_count: int) -> dict:
        """
        Get safety status (green/yellow/red).

        Returns:
            {
                "status": "safe|warning|danger",
                "color": "green|yellow|red",
                "message": "descriptive message"
            }
        """
        limit = self.get_context_limit()
        percentage = (token_count / limit) * 100

        if percentage < 60:
            return {
                "status": "safe",
                "color": "green",
                "message": "Safe - Genug Platz f√ºr Output"
            }
        elif percentage < 85:
            return {
                "status": "warning",
                "color": "yellow",
                "message": "Warning - Context wird knapp"
            }
        else:
            return {
                "status": "danger",
                "color": "red",
                "message": "Danger - Halluzinations-Gefahr!"
            }
```

### Backend: Working State Extension

```python
# Extend working_state with context info

{
  "working_state": {
    "status": "in_progress",
    "progress": 45,
    "current_step": "Executing gap_detector...",
    "started_at": "2026-01-02T10:30:00Z",
    "updated_at": "2026-01-02T10:30:45Z",

    # NEW: Context info
    "context": {
      "loaded_data": [
        {"type": "system_prompt", "tokens": 482},
        {"type": "technique_prompt", "tokens": 356},
        {"type": "previous_output", "name": "competitor_analysis", "tokens": 2104},
        {"type": "user_input", "tokens": 142}
      ],
      "total_tokens": 3084,
      "context_limit": 8192,
      "percentage": 37.6,
      "safety_status": "safe"
    },

    # NEW: Iteration info (for Phase 1)
    "iteration": {
      "current": 2,
      "max": 5,
      "history": [...]
    }
  }
}
```

---

## üé® Frontend: Context Monitoring Component

### ContextMonitor.vue

```vue
<template>
  <div class="context-monitor">
    <div class="context-header">
      <h4>üìä Context Window</h4>
      <span class="model-label">{{ modelName }}</span>
    </div>

    <!-- Progress Bar with Traffic Light -->
    <div class="context-progress">
      <div class="progress-bar">
        <div
          class="progress-fill"
          :class="safetyStatus.color"
          :style="{ width: percentage + '%' }"
        >
          <span class="progress-text">
            {{ totalTokens }} / {{ contextLimit }} tokens ({{ percentage.toFixed(1) }}%)
          </span>
        </div>
      </div>

      <div class="safety-badge" :class="safetyStatus.color">
        {{ safetyStatus.icon }} {{ safetyStatus.message }}
      </div>
    </div>

    <!-- Context Breakdown -->
    <div class="context-breakdown">
      <h5>Geladener Kontext:</h5>
      <div
        v-for="item in loadedData"
        :key="item.type + item.name"
        class="context-item"
      >
        <span class="item-icon">{{ getIcon(item.type) }}</span>
        <span class="item-name">{{ getLabel(item) }}</span>
        <span class="item-tokens">{{ item.tokens }} tokens</span>
      </div>
    </div>

    <!-- Output Buffer Info -->
    <div class="output-buffer">
      <div class="buffer-label">
        Reserved f√ºr Output: {{ outputBuffer }} tokens
      </div>
      <div class="buffer-estimate">
        Gesch√§tzte Max. L√§nge: ~{{ estimateWords(outputBuffer) }} W√∂rter
      </div>
    </div>
  </div>
</template>

<script setup>
import { computed } from 'vue'

const props = defineProps({
  contextInfo: {
    type: Object,
    required: true
  },
  modelName: {
    type: String,
    default: 'mistral-7b-instruct'
  }
})

const totalTokens = computed(() => props.contextInfo.total_tokens || 0)
const contextLimit = computed(() => props.contextInfo.context_limit || 8192)
const percentage = computed(() => props.contextInfo.percentage || 0)
const loadedData = computed(() => props.contextInfo.loaded_data || [])

const outputBuffer = computed(() => {
  // Reserve 25% of context for output
  return Math.floor(contextLimit.value * 0.25)
})

const safetyStatus = computed(() => {
  const status = props.contextInfo.safety_status || 'safe'

  const statusMap = {
    safe: { color: 'green', icon: '‚úÖ', message: 'Safe - Genug Platz f√ºr Output' },
    warning: { color: 'yellow', icon: '‚ö†Ô∏è', message: 'Warning - Context wird knapp' },
    danger: { color: 'red', icon: 'üö®', message: 'Danger - Halluzinations-Gefahr!' }
  }

  return statusMap[status] || statusMap.safe
})

function getIcon(type) {
  const icons = {
    system_prompt: '‚öôÔ∏è',
    technique_prompt: 'üìù',
    previous_output: 'üì§',
    user_input: 'üë§',
    examples: 'üìö'
  }
  return icons[type] || 'üìÑ'
}

function getLabel(item) {
  if (item.name) return item.name

  const labels = {
    system_prompt: 'System Prompt',
    technique_prompt: 'Technique Prompt',
    previous_output: 'Previous Output',
    user_input: 'User Input',
    examples: 'Examples'
  }
  return labels[item.type] || item.type
}

function estimateWords(tokens) {
  // Rough estimate: 1 token ‚âà 0.75 words
  return Math.floor(tokens * 0.75)
}
</script>

<style scoped>
.context-monitor {
  background: var(--bg-panel);
  border: 2px solid var(--border-medium);
  border-radius: 12px;
  padding: 1.5rem;
  margin-bottom: 1rem;
}

.context-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 1rem;
}

.model-label {
  background: var(--accent-gold);
  color: var(--bg-panel);
  padding: 0.25rem 0.75rem;
  border-radius: 6px;
  font-size: 0.85rem;
  font-weight: 600;
}

.context-progress {
  margin-bottom: 1.5rem;
}

.progress-bar {
  height: 40px;
  background: rgba(255, 255, 255, 0.1);
  border-radius: 8px;
  overflow: hidden;
  margin-bottom: 0.5rem;
}

.progress-fill {
  height: 100%;
  display: flex;
  align-items: center;
  justify-content: center;
  transition: width 0.5s ease, background-color 0.3s ease;
  font-weight: 600;
  font-size: 0.9rem;
}

.progress-fill.green {
  background: linear-gradient(90deg, #22c55e, #16a34a);
}

.progress-fill.yellow {
  background: linear-gradient(90deg, #eab308, #f59e0b);
}

.progress-fill.red {
  background: linear-gradient(90deg, #ef4444, #dc2626);
  animation: pulse 1.5s ease-in-out infinite;
}

@keyframes pulse {
  0%, 100% { opacity: 1; }
  50% { opacity: 0.7; }
}

.safety-badge {
  padding: 0.5rem 1rem;
  border-radius: 6px;
  font-weight: 600;
  text-align: center;
}

.safety-badge.green {
  background: rgba(34, 197, 94, 0.2);
  color: #22c55e;
  border: 1px solid #22c55e;
}

.safety-badge.yellow {
  background: rgba(234, 179, 8, 0.2);
  color: #eab308;
  border: 1px solid #eab308;
}

.safety-badge.red {
  background: rgba(239, 68, 68, 0.2);
  color: #ef4444;
  border: 1px solid #ef4444;
}

.context-breakdown {
  margin-bottom: 1.5rem;
}

.context-breakdown h5 {
  color: var(--text-muted);
  font-size: 0.9rem;
  margin-bottom: 0.75rem;
}

.context-item {
  display: flex;
  align-items: center;
  gap: 0.75rem;
  padding: 0.5rem;
  border-bottom: 1px solid var(--border-light);
}

.item-icon {
  font-size: 1.2rem;
}

.item-name {
  flex: 1;
  color: var(--text-light);
}

.item-tokens {
  color: var(--accent-gold);
  font-weight: 600;
  font-family: 'Courier New', monospace;
}

.output-buffer {
  padding: 1rem;
  background: rgba(255, 179, 71, 0.1);
  border: 1px solid var(--accent-gold);
  border-radius: 8px;
}

.buffer-label {
  color: var(--accent-gold);
  font-weight: 600;
  margin-bottom: 0.25rem;
}

.buffer-estimate {
  color: var(--text-muted);
  font-size: 0.9rem;
}
</style>
```

---

## üîÑ Integration in Execution Flow

### 1. Backend: Add to working_state

```python
# src/core/orchestrator.py

async def execute_technique(self, technique_id: str, context: dict):
    # Build prompt
    prompt_parts = {
        "system_prompt": self.system_prompt,
        "technique_prompt": technique.prompt,
        "previous_outputs": [output.content for output in context.get("previous_outputs", [])],
        "user_input": context.get("user_input", ""),
        "examples": technique.get("examples", "")
    }

    # Count tokens
    token_counter = TokenCounter(technique.model)
    context_info = token_counter.estimate_context_size(prompt_parts)

    # Update working state with context info
    technique.working_state.context = {
        "loaded_data": [
            {"type": "system_prompt", "tokens": context_info["breakdown"]["system_prompt"]},
            {"type": "technique_prompt", "tokens": context_info["breakdown"]["technique_prompt"]},
            # ... etc
        ],
        "total_tokens": context_info["total"],
        "context_limit": token_counter.get_context_limit(),
        "percentage": context_info["percentage"],
        "safety_status": token_counter.get_safety_status(context_info["total"])["status"]
    }

    # Execute...
```

### 2. Frontend: Add to ExecutionView

```vue
<template>
  <div class="execution-view">
    <!-- Context Monitor -->
    <ContextMonitor
      v-if="currentExecution?.working_state?.context"
      :context-info="currentExecution.working_state.context"
      :model-name="currentExecution.model"
    />

    <!-- Iteration State (for Phase 1) -->
    <IterationViewer
      v-if="currentExecution?.working_state?.iteration"
      :iteration-state="currentExecution.working_state.iteration"
    />

    <!-- Working State -->
    <WorkingStateViewer :execution-state="currentExecution" />
  </div>
</template>
```

---

## ‚úÖ Success Metrics

- ‚úÖ Token count f√ºr jeden Context-Teil
- ‚úÖ Ampel-Visualisierung (gr√ºn/gelb/rot)
- ‚úÖ Modell-spezifische Limits
- ‚úÖ Echtzeit-Tracking w√§hrend Execution
- ‚úÖ Halluzinations-Warnung bei >85%
- ‚úÖ Reserved Output Buffer anzeigen
- ‚úÖ Iteration State f√ºr Phase 1

---

## üöÄ Next Steps

1. Implement `TokenCounter` class in backend
2. Extend `working_state` schema with `context` field
3. Create `ContextMonitor.vue` component
4. Create `IterationViewer.vue` component
5. Integrate into ExecutionView
6. Test with verschiedenen Models (tier1/tier2/tier3)

---

**Status**: Konzept abgeschlossen ‚Üí Ready for Implementation üöÄ

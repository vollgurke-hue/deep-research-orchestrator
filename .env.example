# llama.cpp Configuration
LLAMA_CPP_PATH=./llama.cpp/llama-cli
MODELS_DIR=./models

# Model Paths
MODEL_FAST=models/Llama-3.1-8B-Instruct-Abliterated-Q6_K.gguf
MODEL_QUALITY=models/Qwen2.5-32B-Instruct-Abliterated-Q4_K_M.gguf

# GPU Configuration
CUDA_VISIBLE_DEVICES=0  # Use only RTX 3060 Ti (8GB)

# Model Settings
FAST_MODEL_GPU_LAYERS=999  # All layers in VRAM
QUALITY_MODEL_GPU_LAYERS=16  # Adjust based on VRAM monitoring

# Server Configuration
FLASK_HOST=127.0.0.1
FLASK_PORT=5000
FLASK_DEBUG=True

# Logging
LOG_LEVEL=INFO
